GAN (Generative Adversarial Network)

1. GAN
- 심층신경망 fully connected network 사용.
- 손실함수 : sigmoid cross entropy loss

2. LSGAN
- D의 손실함수로 cross entropy 대신 least square 사용

3. DCGAN
- 다층 신경망 대신 CNN 사용
- fully connected layer 제거
- 풀링층을 사용하지 않고 합성곱층에서 stride로 그 효과를 대체
- D와 G 모두 batch normalization 사용
- D : leaky ReLU / G : ReLU
- latent space를 이용해 벡터 산술 연산이 가능하기 때문에 의미론적인 속성이 학습됨

4. WGAN
- 손실함수로 Wasserstain divergence 사용
- Discriminator 가 아닌 Critic
- 손실 함수에 로그 함수를 사용하지 않음
- Critic의 가중치가 0 근처에 있도록 clipping 적용
- 모멘텀 기반 Optimizer를 사용하지 않고 RMSProp 사용
- clipping을 개선해 기울기 벡터의 norm이 1에서 멀어지면 벌칙을 주는 WGAN-GP (gradient penalty)
* 참조 : https://haawron.tistory.com/21

5. Conditional GAN
- group label 등의 보조 정보 활용
- 원하는 특징을 가진 이미지를 생성하도록 조정
- fake data에 제공되는 조건은 단지 데이터에 결합될 뿐
- real data에서는 조건에 해당하는 x를 랜덤 추출하여 결합
- pix2pix, styleGAN의 기초

6. InfoGAN
- GAN에 정보이론을 결합
- 두 분포의 상호정보량(mutual information)을 통해 상호의존성의 크기를 측정
- noise vector (z)에 latent code (c)를 추가
- 예: c1 숫자 종류, c2 숫자 기울기 등
- 보조분포 Q를 정의해 상호정보량의 lower bound를 계산
- 손실함수에 상호정보량 계산이 추가되어 c가 데이터의 특징을 가지게 된다.
* 참조 : https://haawron.tistory.com/10

7. CycleGAN & pix2pix
- 두 도메인 양쪽에 대응하는 데이터 쌍이 없어도 한 도메인에 있는 영상을 다른 도메인으로 해석 가능
- pix2pix : x와 y 두 쌍의 이미지가 쌍으로 제공됨
- CycleGAN : 이미지 쌍이 아닌 서로 다른 두 도메인의 이미지 사용
- noise vector (z) 사용하지 않음

8. AC (Auxiliary classifier) GAN
- 생성된 모든 표본에 group label 부여

9. DiscoGAN
- 서로 다른 도메인 사이의 관계를 찾는 것을 학습


-------------------------------------------------------------------
<게임이론 기초>

1. 최소극대화 maximin / 최대극소화 minimax

(1) 최소극대화
게임 참가자 i에게 보장되는 최소 이익이 가장 큰 전략을 선택.
즉, 참가자 i가 다른 참가자들의 전략을 전혀 알지 못해도 확실히 보장되는 이익의 최대값.

(2) 최대극소화
다른 참가자가 얻을 수 있는 최대 이익이 가장 작은 전략을 선택.


2. Zero-sum game

모든 참가자의 이익 또는 손실을 전부 합하면 0이 되는 게임.
2인 제로섬 게임의 경우 내시 균형에서 각 참가자가 얻게 되는 이익은 항상 최대극소값, 최소극대값과 같다.


-------------------------------------------------------------------
<GAN의 이익함수 & 함수>

1. Vanilla GAN의 이익함수

GAN의 생성자(G)와 판별자(D)는 제로섬 게임의 최소극대화 전략을 선택해 서로 이기려고 노력한다.

* 판별자 이익함수 --> 최대화

    U(D, G) = [log D(x)] + [log (1 - D(G(z)))]

* 생성자 이익함수 --> 최대화

    V(D, G) = - [log D(x)] - [log (1 - D(G(z)))]


2. 학습 알고리즘

for n_epoch:
  * 노이즈 분포에서 m 개의 표본 추출 (A)
  * 데이터 분포에서 m 개의 표본 추출 (B)
  * 판별자 학습 : A --> False & B --> True

  * 노이즈 분포에서 m 개의 표본 추출 (C)
  * 생성자 학습


3. 생성자의 vanishing gradient problem

  -[log (1 - D(G(z)))] 를 최소화하는 대신에 -[log G(z)]를 최소화하도록 학습.


4. Vanilla GAN의 문제점

  * gradient descent의 비수렴성 - G와 D의 균형을 확보하기 어려움.
  * mode collapse - G가 같은 데이터만 생성.
  * vanishing gradient - D의 학습이 너무 성공적임.
  * hyperparameter에 매우 민감.


-------------------------------------------------------------------
두 분포의 거리(차이)를 나타내는 정보량

--> 신경망에 적용하면 특정 분포에 근사하는 매우 복잡한 분포를 학습할 수 있음.

* KL (Kullback Leibier) divergence
  - 어떤 분포를 근사하는 다른 분포를 사용해 샘플링을 할 때 손실될 수 있는 정보 엔트로피 차이를 계산.
  - 비대칭으로 두 값의 위치를 바꾸면 함수값도 달라지기 때문에 거리 함수가 아님.

* JS (Jensen Shannon) divergence
  -  KL-Divergence를 symmetric하게 개량하여 두 분포 사이의 거리(distance)로서 사용 가능.

* Pearson chi-squre divergence

* Total variation

* Wasserstein divergence
  - 두 분포를 일치시키기 위해 필요한 최소 운동량으로 해석할 수 있다.
  - KL 또는 JS divergence 보다 두 분포의 거리를 잘 표현한다.

-------------------------------------------------------------------
