GAN (Generative Adversarial Network)

1. GAN
- 심층신경망 fully connected network 사용.
- 손실함수 : sigmoid cross entropy loss

2. LSGAN
- D의 손실함수로 cross entropy 대신 least square 사용

3. DCGAN
- 다층 신경망 대신 CNN 사용
- fully connected layer 제거
- 풀링층을 사용하지 않고 합성곱층에서 stride로 그 효과를 대체
- D와 G 모두 batch normalization 사용
- D : leaky ReLU / G : ReLU

4. WGAN
- 손실함수로 Wasserstain divergence 사용

5. Conditional GAN
- group label 등의 보조 정보 활용


6. InfoGAN
- GAN에 정보이론을 결합

7. CycleGAN
- 두 도메인 양쪽에 대응하는 데이터 쌍이 없어도 한 도메인에 있는 영상을 다른 도메인으로 해석 가능

8. AC (Auxiliary classifier) GAN
- 생성된 모든 표본에 group label 부여

9. DiscoGAN
- 서로 다른 도메인 사이의 관계를 찾는 것을 학습


-------------------------------------------------------------------
<게임이론 기초>

1. 최소극대화 maximin / 최대극소화 minimax

(1) 최소극대화
게임 참가자 i에게 보장되는 최소 이익이 가장 큰 전략을 선택.
즉, 참가자 i가 다른 참가자들의 전략을 전혀 알지 못해도 확실히 보장되는 이익의 최대값.

(2) 최대극소화
다른 참가자가 얻을 수 있는 최대 이익이 가장 작은 전략을 선택.


2. Zero-sum game

모든 참가자의 이익 또는 손실을 전부 합하면 0이 되는 게임.
2인 제로섬 게임의 경우 내시 균형에서 각 참가자가 얻게 되는 이익은 항상 최대극소값, 최소극대값과 같다.


-------------------------------------------------------------------
<GAN의 이익함수 & 함수>

1. Vanilla GAN의 이익함수

GAN의 생성자(G)와 판별자(D)는 제로섬 게임의 최소극대화 전략을 선택해 서로 이기려고 노력한다.

* 판별자 이익함수 --> 최대화

    U(D, G) = [log D(x)] + [log (1 - D(G(z)))]

* 생성자 이익함수 --> 최대화

    V(D, G) = - [log D(x)] - [log (1 - D(G(z)))]


2. 학습 알고리즘

for n_epoch:
  * 노이즈 분포에서 m 개의 표본 추출 (A)
  * 데이터 분포에서 m 개의 표본 추출 (B)
  * 판별자 학습 : A --> False & B --> True

  * 노이즈 분포에서 m 개의 표본 추출 (C)
  * 생성자 학습


3. 생성자의 vanishing gradient problem

  -[log (1 - D(G(z)))] 를 최소화하는 대신에 -[log G(z)]를 최소화하도록 학습.


4. Vanilla GAN의 문제점

  * gradient descent의 비수렴성 - G와 D의 균형을 확보하기 어려움.
  * mode collapse - G가 같은 데이터만 생성.
  * vanishing gradient - D의 학습이 너무 성공적임.
  * hyperparameter에 매우 민감.


-------------------------------------------------------------------
두 분포의 거리(차이)를 나타내는 정보량

--> 신경망에 적용하면 특정 분포에 근사하는 매우 복잡한 분포를 학습할 수 있음.

* KL (Kullback Leibier) divergence
  - 어떤 분포를 근사하는 다른 분포를 사용해 샘플링을 할 때 손실될 수 있는 정보 엔트로피 차이를 계산.
  - 비대칭으로 두 값의 위치를 바꾸면 함수값도 달라지기 때문에 거리 함수가 아님.

* JS (Jensen Shannon) divergence
  -  KL-Divergence를 symmetric하게 개량하여 두 분포 사이의 거리(distance)로서 사용 가능.

* Pearson chi-squre divergence

* Total variation

* Wasserstein divergence


-------------------------------------------------------------------
